import os
import numpy as np
import torch
import matplotlib.pyplot as plt
import math
from utils.augmentation import *

plt.switch_backend('agg')

def validate_window_config(data_freq, seq_len, pred_len):
    """éªŒè¯çª—å£é…ç½®çš„åˆç†æ€§ - æ”¯æŒ30åˆ†é’Ÿç²’åº¦"""
    
    # é¢‘ç‡åˆ°åˆ†é’Ÿæ•°çš„æ˜ å°„
    freq_to_minutes = {
        't': 15,    # 15åˆ†é’Ÿ
        't30': 30,  # 30åˆ†é’Ÿ
        'h': 60,    # 1å°æ—¶  
        'd': 1440   # 1å¤©
    }
    
    if data_freq not in freq_to_minutes:
        print(f"âš ï¸ æœªçŸ¥é¢‘ç‡: {data_freq}, è¯·ä½¿ç”¨ {list(freq_to_minutes.keys())}")
        return
    
    minutes_per_point = freq_to_minutes[data_freq]
    
    # è®¡ç®—ç‰©ç†æ—¶é—´è·¨åº¦
    input_minutes = seq_len * minutes_per_point
    pred_minutes = pred_len * minutes_per_point
    
    input_hours = input_minutes / 60
    pred_hours = pred_minutes / 60
    
    input_days = input_hours / 24
    pred_days = pred_hours / 24
    
    print(f"æ•°æ®ç²’åº¦: {data_freq} ({minutes_per_point}åˆ†é’Ÿ/ç‚¹)")
    print(f"è¾“å…¥è·¨åº¦: {input_minutes}åˆ†é’Ÿ = {input_hours:.1f}å°æ—¶ = {input_days:.1f}å¤©")
    print(f"é¢„æµ‹è·¨åº¦: {pred_minutes}åˆ†é’Ÿ = {pred_hours:.1f}å°æ—¶ = {pred_days:.1f}å¤©")
    
    # æ£€æŸ¥æ˜¯å¦è¦†ç›–å…³é”®å‘¨æœŸ
    print("\nå‘¨æœŸæ€§è¦†ç›–æ£€æŸ¥:")
    if input_hours >= 24:
        print("âœ“ è¦†ç›–æ—¥å‘¨æœŸ")
    if input_hours >= 168:  
        print("âœ“ è¦†ç›–å‘¨å‘¨æœŸ")
    if input_days >= 30:
        print("âœ“ è¦†ç›–æœˆå‘¨æœŸ")
    
    # é’ˆå¯¹30åˆ†é’Ÿçš„ç‰¹æ®Šå»ºè®®
    if data_freq == 't30':
        print("\nğŸ” 30åˆ†é’Ÿç²’åº¦ç‰¹æ®Šå»ºè®®:")
        if seq_len % 48 == 0:
            print("âœ“ è¾“å…¥é•¿åº¦æ˜¯48çš„å€æ•°ï¼Œèƒ½å®Œæ•´å¯¹é½æ—¥å‘¨æœŸ")
        else:
            print("âš ï¸ å»ºè®®è°ƒæ•´è¾“å…¥é•¿åº¦ä¸º48çš„å€æ•°ä»¥æ›´å¥½å¯¹é½æ—¥å‘¨æœŸ")
            
        if pred_len % 48 == 0:
            print("âœ“ é¢„æµ‹é•¿åº¦æ˜¯48çš„å€æ•°ï¼Œèƒ½å®Œæ•´å¯¹é½æ—¥å‘¨æœŸ")
        else:
            print("âš ï¸ å»ºè®®è°ƒæ•´é¢„æµ‹é•¿åº¦ä¸º48çš„å€æ•°ä»¥æ›´å¥½å¯¹é½æ—¥å‘¨æœŸ")

def adjust_learning_rate(optimizer, epoch, args):
    """
    æ ¹æ®ä¸åŒçš„ç­–ç•¥è°ƒæ•´å­¦ä¹ ç‡ã€‚
    
    æœ¬å‡½æ•°æ ¹æ®å½“å‰çš„è®­ç»ƒå‘¨æœŸï¼ˆepochï¼‰å’Œé…ç½®å‚æ•°ï¼ˆargsï¼‰æ¥åŠ¨æ€è°ƒæ•´ä¼˜åŒ–å™¨ï¼ˆoptimizerï¼‰çš„å­¦ä¹ ç‡ã€‚
    æ”¯æŒä¸‰ç§ç±»å‹çš„å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥ï¼š'type1', 'type2', å’Œ 'cosine'ï¼Œé€šè¿‡args.lradjå‚æ•°æŒ‡å®šã€‚
    
    å‚æ•°:
    optimizer: ä¼˜åŒ–å™¨ï¼Œå…¶å­¦ä¹ ç‡éœ€è¦è°ƒæ•´ã€‚
    epoch: å½“å‰çš„è®­ç»ƒå‘¨æœŸæ•°ã€‚
    args: åŒ…å«è°ƒæ•´ç­–ç•¥å’Œå¯èƒ½çš„å…¶ä»–ç›¸å…³é…ç½®çš„å‚æ•°ã€‚
    
    è¿”å›:
    æ— è¿”å›å€¼ï¼Œç›´æ¥ä¿®æ”¹ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡å¹¶æ‰“å°æ›´æ–°åçš„å­¦ä¹ ç‡ã€‚
    """
    # æ ¹æ®'args.lradj'å‚æ•°å€¼é€‰æ‹©ä¸åŒçš„å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥
    if args.lradj == 'type1':
        # æŒ‰ç…§æ¯1ä¸ªå‘¨æœŸå­¦ä¹ ç‡å‡åŠçš„ç­–ç•¥è°ƒæ•´å­¦ä¹ ç‡
        lr_adjust = {epoch: args.lr * (0.5 ** ((epoch - 1) // 1))}
    elif args.lradj == 'type2':
        # æŒ‰ç…§é¢„è®¾çš„å‘¨æœŸç‚¹é™ä½å­¦ä¹ ç‡
        lr_adjust = {
            2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,
            10: 5e-7, 15: 1e-7, 20: 5e-8
        }
    elif args.lradj == "cosine":
        # æŒ‰ç…§ä½™å¼¦é€€ç«ç­–ç•¥è°ƒæ•´å­¦ä¹ ç‡
        lr_adjust = {epoch: args.lr / 2 * (1 + math.cos(epoch / args.train_epochs * math.pi))}
    
    # æ£€æŸ¥å½“å‰å‘¨æœŸæ˜¯å¦åœ¨è°ƒæ•´å­—å…¸ä¸­ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™æ›´æ–°å­¦ä¹ ç‡
    if epoch in lr_adjust.keys():
        lr = lr_adjust[epoch]
        # æ›´æ–°ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr
        # æ‰“å°å­¦ä¹ ç‡æ›´æ–°ä¿¡æ¯
        print('Updating learning rate to {}'.format(lr))



class IterationEarlyStopping:
    def __init__(self, patience_epochs=10, patience_iterations=50, min_iterations=500, 
                 verbose=True, delta=0, mode='min'):
        """
        æ··åˆæ—©åœæœºåˆ¶ï¼šiterationçº§åˆ«åˆ¤æ–­ + epochçº§åˆ«ä¿å­˜
        
        Args:
            patience_epochs: epochçº§åˆ«çš„å®¹å¿æ•°
            patience_iterations: iterationçº§åˆ«çš„å®¹å¿æ•°  
            min_iterations: æœ€å°‘è®­ç»ƒiterationæ•°ï¼ˆé¿å…è¿‡æ—©åœæ­¢ï¼‰
            verbose: æ˜¯å¦æ‰“å°ä¿¡æ¯
            delta: æœ€å°æ”¹å–„é˜ˆå€¼
            mode: 'min'è¡¨ç¤ºæœ€å°åŒ–æŸå¤±ï¼Œ'max'è¡¨ç¤ºæœ€å¤§åŒ–æŒ‡æ ‡
        """
        self.patience_epochs = patience_epochs
        self.patience_iterations = patience_iterations
        self.min_iterations = min_iterations
        self.verbose = verbose
        self.delta = delta
        self.mode = mode
        
        self.counter_epochs = 0
        self.counter_iterations = 0
        self.early_stop = False
        self.val_loss_min = np.inf
        self.global_iteration = 0

        # åˆ†åˆ«ç»´æŠ¤iterationçº§åˆ«å’Œepochçº§åˆ«çš„æœ€ä½³åˆ†æ•°
        self.best_score_iter = None  # iterationçº§åˆ«æœ€ä½³åˆ†æ•°
        self.best_score_epoch = None  # epochçº§åˆ«æœ€ä½³åˆ†æ•°
        
        # ç”¨äºiterationçº§åˆ«ç›‘æ§çš„æ»‘åŠ¨çª—å£
        self.loss_window = []
        self.window_size = 20  # ç›‘æ§æœ€è¿‘20ä¸ªiterationçš„æŸå¤±

    def __call__(self, current_loss, model,  full_model_path=None , is_iteration=False, current_iteration=0):
        """
        è°ƒç”¨æ—©åœåˆ¤æ–­
        
        Args:
            current_loss: å½“å‰æŸå¤±å€¼
            model: æ¨¡å‹
            path: ä¿å­˜è·¯å¾„
            is_iteration: æ˜¯å¦ä¸ºiterationçº§åˆ«è°ƒç”¨
            current_iteration: å½“å‰å…¨å±€iterationæ•°
        """
        self.global_iteration = current_iteration
        
        if self.mode == 'min':
            score = -current_loss
        else:
            score = current_loss
        
        
        if is_iteration: # iterationçº§æ—©åœè°ƒç”¨å¤„ç†
            if self.best_score_iter is None: # ç¬¬ä¸€æ¬¡ iterationçº§æ—©åœè°ƒç”¨
                self.best_score_iter = score
            # iterationçº§åˆ«çš„æ”¹å–„åˆ¤æ–­
            improved = score > self.best_score_iter + self.delta 
            if improved: # æœ‰æ”¹å–„ï¼šæ›´æ–°æœ€ä½³åˆ†æ•°ï¼Œé‡ç½®è®¡æ•°å™¨
                self.best_score_iter = score
                self.counter_iterations = 0
            else:
                self.counter_iterations += 1 # æ— æ”¹å–„ï¼šå¢åŠ  iteration è®¡æ•°å™¨
        else: # epoch çº§æ—©åœè°ƒç”¨å¤„ç†
            if self.best_score_epoch is None: # ç¬¬ä¸€æ¬¡ epochçº§æ—©åœè°ƒç”¨,ä¸€å®šä¼šä¿å­˜æ¨¡å‹
                self.best_score_epoch = score
                if full_model_path is not None: # ç¬¬ä¸€æ¬¡epochçº§åˆ«è°ƒç”¨æ—¶ä¿å­˜æ¨¡å‹
                    self.save_checkpoint(current_loss, model, full_model_path)
            else:
                # epochçº§åˆ«çš„æ”¹å–„åˆ¤æ–­
                improved = score > self.best_score_epoch # epochçº§åˆ«æ˜¯å¦æ”¹å–„æ”¾å®½æ¾ç‚¹ï¼Œä¸éœ€è¦deltaåç¦»å€¼
                if improved:
                    self.best_score_epoch = score
                    self.counter_epochs = 0
                    if full_model_path is not None:
                        self.save_checkpoint(current_loss, model, full_model_path)
                    else:
                        self.counter_epochs += 1
        
        # åˆ¤æ–­æ˜¯å¦æ—©åœ
        stop_by_epoch = self.counter_epochs >= self.patience_epochs
        stop_by_iteration = (self.counter_iterations >= self.patience_iterations and 
                           self.global_iteration >= self.min_iterations)
        
    
        if stop_by_epoch or stop_by_iteration :
            self.early_stop = True
            if self.verbose:
                if stop_by_epoch:
                    print(f'Early stopping: {self.counter_epochs} epochs without improvement')
                else:
                    print(f'Early stopping: {self.counter_iterations} iterations without improvement '
                          f'(total: {self.global_iteration} iterations)')
                    

    def update_loss_window(self, loss):
        """æ›´æ–°æŸå¤±æ»‘åŠ¨çª—å£"""
        self.loss_window.append(loss)
        if len(self.loss_window) > self.window_size:
            self.loss_window.pop(0)
    
    def is_loss_stable(self, threshold=0.001):
        """åˆ¤æ–­æŸå¤±æ˜¯å¦è¶‹äºç¨³å®š"""
        if len(self.loss_window) < self.window_size:
            return False
        
        recent_losses = np.array(self.loss_window)
        # è®¡ç®—æœ€è¿‘çª—å£å†…æŸå¤±çš„å˜å¼‚ç³»æ•°
        if np.mean(recent_losses) > 0:
            cv = np.std(recent_losses) / np.mean(recent_losses)
            return cv < threshold
        return False

    def save_checkpoint(self, val_loss, model, full_model_path):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        if self.verbose:
            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')
            torch.save(model.state_dict(), full_model_path)
        self.val_loss_min = val_loss

class dotdict(dict):
    """dot.notation access to dictionary attributes"""
    __getattr__ = dict.get
    __setattr__ = dict.__setitem__
    __delattr__ = dict.__delitem__


class StandardScaler():
    def __init__(self, mean, std):
        self.mean = mean
        self.std = std

    def transform(self, data):
        return (data - self.mean) / self.std

    def inverse_transform(self, data):
        return (data * self.std) + self.mean


def visual(true, preds=None, name='./pic/test.pdf'):
    """
    Results visualization
    """
    plt.figure()
    plt.plot(true, label='GroundTruth', linewidth=2)
    if preds is not None:
        plt.plot(preds, label='Prediction', linewidth=2)
    plt.legend()
    plt.savefig(name, bbox_inches='tight')


